
import torch
import torchvision

import einops
from PIL import Image
from tqdm import tqdm

from dataset import Glint360KSubset_Base


class Glint360KSubset_Occlusion(Glint360KSubset_Base):

    def __init__(
            self, 
            trans, 
            loc,
            mpx,
            image_size):
        
        super().__init__(trans, loc)

        masks = torch.zeros((image_size//mpx) ** 2, (image_size//mpx) ** 2)
        for i in range(masks.shape[0]):
            masks[i][i] = 1.
        masks = einops.rearrange(masks, 'b (l w) -> b l w', l=(image_size//mpx))
        self.masks = einops.repeat(masks, 'b l w -> b 3 (l c1) (w c2)', c1=mpx, c2=mpx)

    def __getitem__(self, x):

        image_loc = self.items[x]
        image = self.trans(Image.open(image_loc).convert("RGB"))

        image_occluded = einops.repeat(image, 'c h w -> r c h w', r=self.masks.size(0))
        image_occluded = torch.where(self.masks == 1., -1., image_occluded)

        return image_loc, image, image_occluded
    

def generate_occlusion_scores(
        fr_model: torch.nn.Module, 
        trans: torchvision.transforms.Compose, 
        loc: str, 
        batch_size: int, 
        sim_func: torch.nn.Module, 
        mpx: int, 
        image_size: int) -> dict:
    """Generates quality scores using occlusions as the perturbation of choice.

    Args:
        fr_model (torch.nn.Module): Face Recognition used to generate the pseudo quality labels.
        trans (torchvision.transforms.v2.Compose): Function that prepares images for input to the FR model.
        loc (str): Location of the dataset used to generate the labels over.
        batch_size (int): Batch size.
        sim_func (torch.nn.Module): Similarity function used to compare the features of the original and perturbed images.
        mpx (int): Pixel size of the occluded area.
        image_size (int): Size of images to occlude.

    Returns:
        dict: {"Image path": quality_score} values generated by the occlusion perturbation. 
    """

    # Prepare FR model
    fr_model.cuda().eval()

    # Get occlusion specific dataset and construct dataloader
    dataset = Glint360KSubset_Occlusion(trans, loc, mpx, image_size)
    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size)

    occlusion_results = {}
    # Generate results by comparing original to occluded images
    with torch.no_grad(), torch.cuda.amp.autocast():

        for (image_loc_batch, image_batch, image_batch_occluded) in tqdm(dataloader, "Generating Occlusion Scores: "):

                all_embs = fr_model(
                    torch.cat((
                            image_batch,
                            einops.rearrange(image_batch_occluded, 'b1 b2 c h w -> (b1 b2) c h w')
                            )
                         ).cuda()).detach()

                base_embs = all_embs[:image_batch.size(0)]
                occluded_embs = einops.rearrange(
                    all_embs[image_batch.size(0):], '(b1 b2) c -> b1 b2 c', b1=image_batch_occluded.size(0))
                sims = torch.mean(
                    torch.nn.functional.cosine_similarity(
                        einops.repeat(base_embs, 'b c -> b b2 c', b2=image_batch_occluded.size(1)), 
                        occluded_embs, dim=2), 
                    dim=1)

                occlusion_results.update(dict(zip(image_loc_batch, sims.detach().cpu().numpy().tolist())))

    return occlusion_results

